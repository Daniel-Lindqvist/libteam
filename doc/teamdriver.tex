\documentclass[11pt,a4paper]{article}

\oddsidemargin -7mm
\textwidth 17cm
\textheight 20cm

\begin{document}
\title{Team driver (for developers)}
\author{Jiri Pirko\\
\texttt{jpirko@redhat.com}\\
\texttt{http://www.libteam.org}}

\maketitle
\newpage

\section{Prologue}
The primary purpose of this document is to give an introduction to the codebase of the Team driver project. Target audience should have a knowledge about Linux kernel development in general with focus on Linux kernel network core and drivers.

Purpose of the Team driver is to provide a mechanism to team multiple NICs (ports) into one logical one (teamdev). This is already implemented in Linux kernel by bonding driver. Main thing to realize is that Team driver project does not try to replicate or mimic bonding driver. What it does is it resolves the same problem using a different perspective. Therefore for example the way Team is configured differs dramatically from the way bonding is.

This document cuts the whole Team driver project into its pieces describing each one in details. User should be able to pick up these pieces together to assemble the whole picture.

\section{Kernel Part}

The kernel part of Team driver project, the Team driver, is designed to be as slim as possible. The motto is: "If something can be done in userspace, do it in userspace". So only fast paths (TX and RX packet processing) and necessary parts as netdev and Netlink API are implemented in kernel. In fact the kernel part on its own can't do much, userspace puppeteer is needed! The driver is present in upstream Linux kernel since version 3.3. The code is divided into files described in table \ref{table1}

\begin{table}[ht]
  \begin{tabular}{ l l }
    \verb+include/linux/if_team.h+ & Mode modules API, userspace API  \\
    \verb+drivers/net/team_mode_*.c+ & Mode modules implementations \\
    \verb+drivers/net/team/team.c+ & Everything else \\
  \end{tabular}
  \caption{Team driver files}
  \label{table1}
\end{table}

\subsection{Netdev implementation}

Driver allows to create its instance using RTNL (by registering \verb+struct rtnl_link_ops+). Also, it uses netdevice notification facility to catch up events happening on ports to be able to perform needed actions in reaction to that. Events such as port link change, port dissapearance and so on.

When an instance of Team is created (call it for example \verb+team0+) it looks like any other network device. So you can perform any action on it as if it was ordinary network device. The difference is that it is not by itself able to receive or transmit \verb+skbs+. It uses another network devices (typically ones representing real NICs) to do that. Team registers its own set of netdev ops. See \verb+struct net_device_ops team_netdev_ops+ what ops are registered.

The \verb+skb+ is passed from net core to Team driver by calling \verb+->ndo_start_xmit()+ which is in case of Team driver \verb+team_xmit()+ function. In that function, mode handler of transmit function is called and its logic selects the port to be used to transmit the \verb+skb+ . The \verb+skb->dev+ is then changed to the selected port netdev (for example from \verb+team0+ to \verb+eth0+) and the \verb+skb+ is passed back to net core by calling \verb+dev_queue_xmit()+. The mentioned change of \verb+skb->dev+ ensures that the skb will be passed to port netdev's \verb+->ndo_start_xmit()+.

On receive path, Team driver exploits \verb+rx_handler+ hook facility by registering \verb+team_handle_frame()+ to intercept incoming \verb+skbs+. Receive handler of mode is called to process the packet. In most cases, \verb+skb->dev+ is changed to Team instance netdev (for example from \verb+eth0+ to \verb+team0+) and \verb+RX_HANDLER_ANOTHER+ is returned signaling that \verb+skb+ should be re-inserted into net core RX processing path. When for example port is not enabled, \verb+RX_HANDLER_EXACT+ is returned which means the \verb+skb+ is delivered only to sockets bind directly to the port netdevice.

If the selected mode does not provide transmit or receive handlers, the dummy ones (they do nothing) are used to speed up the packet processing (one \verb+if+ and check for \verb+NULL+ are avoided). Function \verb+team_adjust_ops()+ sets dummy handlers in case mode does not provide its own.

Ports are added or removed using \verb+ndo_add_slave+ and \verb+ndo_del_slave+. Note that Team driver supports only Ethernet at the moment. But plan is to support other types in future. Whole code (both kernel and userspace) is written in accordance to that.

\subsection{Netlink API}

The only interface which can be used to communicate with Team driver instance from userspace (aside from standard APIs as RTNL and ethtool and netdev sysfs) is Team Netlink API. It's implemented using Generic Netlink.

There are two types of netlink data being communicated. Ports info and options. Read-only ports info contains speed, duplex and linkstate taken direcly from ethtool (see \verb+__team_port_change_check()+). Options may be of multiple types (\verb+enum team_option_type+), may be per-port, may be array. Option inner interface towards the rest Team driver code and mode modules is defined by \verb+struct team_option+. Options are registered (by mode modules for example) via \verb+team_options_register()+ function. Think of an option instance as of an object instance with functions to get (getter) and set (setter) its value. Team Netlink core calls function \verb+team_option->getter()+ in order to get option value and \verb+team_option->getter()+ in order to set option value.

There are following kinds of communition taking place on Team Generic Netlink:

\begin{enumerate}
\item Userspace application requests options values or ports info. Driver assembles the message and send that back.
\item Userspace application requests to change value of an option. Driver processes the request and returns error code back to the application
\item Whenever status of any of ports gets changed or any option value gets changed, driver assembles a message (similar to the one noted in point 1.) and send that to userspace as multicast Netlink message.
\end{enumerate}

\subsection{Team modes}

Modes are implemented as separate modules. Modes (defined by \verb+struct team_mode+) implement handlers (\verb+struct team_mode_ops+) which are called from Team core. These handlers defines the behaviour. Currently there are three modes defined:

\begin{enumerate}
\item \verb+roundrobin+ - very basic mode with RX port selector based on looping around the port list.
\item \verb+activebackup+ - exposes \verb+activeport+ option through which userspace application can specify port to be used for TX and RX.
\item \verb+loadbalance+ - more complex mode used for LACP and TX/RX loadbalancing. Hashes are used to identify similar packets. Hash computation mechanism is BPF based and can be set-up via \verb+bpf_hash_func\+ option.
\end{enumerate}

\section{Userspace part}

The userspace part of the code can be get from following URL: \verb+https://github.com/jpirko/libteam+. The overall name for userspace part is \verb+Libteam+. It contains Libteam library but it contains other code as well. The code is divided into files described in table \ref{table2}

\begin{table}[ht]
  \begin{tabular}{ l l }
    \verb+lib/*+ & Libteam library (Netlink wrap-up)  \\
    \verb+binding/*+ & Currently only Python binding for Libteam library \\
    \verb+include/team.h+ & Libteam library header \\
    \verb+include/linux/*+ & Kernel headers \\
    \verb+include/private/*+ & Private Libteam headers \\
    \verb+src/*+ & Couple of example applications using Libteam library \\
    \verb+examples/*+ & Couple of example applications using Libteam library Python binding \\
    \verb+man/*+ & Manual pages \\
    \verb+teamd/example_configs/*+ & Example Teamd configs \\
    \verb+teamd/teamd/teamd_runner+*+ & Teamd runners \\
    \verb+teamd/teamd/*+ & Teamd deamon core (rest of the files) \\
  \end{tabular}
  \caption{Libteam files}
  \label{table2}
\end{table}

\subsection{Libteam library}

This library does userspace wrapping of Team Generic Netlink messages. Also, it wraps-up RT Netlink messages (such as \verb+newlink+, \verb+dellink+ etc.). Among others, the lib allows to do following important things:

\begin{itemize}
\item init communitacion and create, destroy team netdevice (\verb+team_alloc()+, \verb+team_create()+, \verb+team_init()+, \verb+team_destroy()+, etc.)
\item register handlers to get port and option changes (\verb+struct team_change_handler+). This allows user to register a function handler called whenever desired event happen.
\item adding and removing port (\verb+team_port_add()+, \verb+team_port_remove()+)
\item get ports info (iteration macro \verb+team_for_each_port()+ and port getters)
\item get and set options (iteration macro \verb+team_for_each_option()+ and option getters and setters)
\end{itemize}

There is also Python binding for this lib available.

\subsection{Teamd daemon}

Teamd is an application which is a part of Libteam project and it's using Libteam library. It runs as a daemon and one instance of Teamd is working with one instance of Team driver (netdev). It's configured by JSON config files. There are some examples in \verb+teamd/example_configs/+ directory.

When the application starts it reads config (either from file or passed as command line parameter). It Creates Team driver instance by given name, adds ports to that and sets up runner and link-watch.

All the runtime work is done in main loop (\verb+teamd_run_loop_run+). So whatever runner or link-watch need to do on they own (like periodic work or socket data receive) it must register \verb+struct teamd_loop_callback+ for that.

\subsubsection{Runners}

Runners determines the behaviour of the device. They operate using a kernel Team mode they want. Runners watch for port state changes and react to that. Also they may implement other functionality. Following runners are implemented:

\begin{itemize}
\item \verb+dummy+ 
\item \verb+roundrobin+ 
\item \verb+activebackup+
\item \verb+lacp+ - implements 802.3ad LACP protocol
\item \verb+loadbalance+
\end{itemize}

\subsubsection{Link-watches}

Link-watches serves for link monitoring purposes. Runner uses function \verb+teamd_link_watch_set_handler()+ to set a handler. This handler is called whenever link-watch decides that link change occured. The decision depends on link-watch type used. There are currently following types:

\begin{itemize}
\item \verb+ethtool+ - Uses Libteam library to get port ethtool state changes. 
\item \verb+arp_ping+ - Each port sends ARP requests. If ARP reply is received, link is considered to be up.
\item \verb+nsna_ping+ - Similar to the previous, only it uses IPV6 Neighbor Solicitation / Neighbor Announcement mechanism.
\end{itemize}

\end{document}
